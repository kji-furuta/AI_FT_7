<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>利用マニュアル - AI Fine-tuning Toolkit</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.0/font/bootstrap-icons.css" rel="stylesheet">
    <style>
        body {
            background-color: #f8f9fa;
            padding-top: 60px;
        }
        .manual-section {
            background: white;
            border-radius: 10px;
            padding: 30px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .warning-box {
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
        }
        .info-box {
            background-color: #d1ecf1;
            border: 1px solid #bee5eb;
            border-radius: 5px;
            padding: 15px;
            margin: 20px 0;
        }
        .model-status {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 3px;
            font-size: 0.85em;
            margin-left: 10px;
        }
        .status-available {
            background-color: #28a745;
            color: white;
        }
        .status-download-required {
            background-color: #ffc107;
            color: #000;
        }
        .toc {
            position: sticky;
            top: 80px;
        }
        h2 {
            margin-top: 30px;
            padding-bottom: 10px;
            border-bottom: 2px solid #dee2e6;
        }
        h3 {
            margin-top: 20px;
            color: #495057;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="/">
                <i class="bi bi-robot"></i> AI Fine-tuning Toolkit
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav ms-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="/">ダッシュボード</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link active" href="/manual">利用マニュアル</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="/system-overview">システム概要</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container mt-4">
        <div class="row">
            <!-- 目次 -->
            <div class="col-lg-3">
                <div class="toc">
                    <h5>目次</h5>
                    <nav class="nav flex-column">
                        <a class="nav-link" href="#introduction">はじめに</a>
                        <a class="nav-link" href="#available-models">利用可能なモデル</a>
                        <a class="nav-link" href="#model-download">モデルのダウンロード</a>
                        <a class="nav-link" href="#finetuning">ファインチューニング</a>
                        <a class="nav-link" href="#text-generation">テキスト生成</a>
                        <a class="nav-link" href="#troubleshooting">トラブルシューティング</a>
                    </nav>
                </div>
            </div>

            <!-- メインコンテンツ -->
            <div class="col-lg-9">
                <h1>AI Fine-tuning Toolkit 利用マニュアル</h1>

                <section id="introduction" class="manual-section">
                    <h2><i class="bi bi-book"></i> はじめに</h2>
                    <p>AI Fine-tuning Toolkitは、日本語大規模言語モデル（LLM）のファインチューニングを効率的に行うためのWebベースのツールです。</p>
                    
                    <div class="warning-box">
                        <i class="bi bi-exclamation-triangle-fill"></i> <strong>重要なお知らせ</strong><br>
                        現在、実際にダウンロード済みで即座に利用可能なモデルは<strong>4つのみ</strong>です。その他のモデルは初回使用時に自動的にダウンロードされますが、大規模モデルはダウンロードに時間がかかる場合があります。
                    </div>
                </section>

                <section id="available-models" class="manual-section">
                    <h2><i class="bi bi-cpu"></i> 利用可能なモデル</h2>
                    
                    <h3>ダウンロード済みモデル（即座に利用可能）</h3>
                    <div class="table-responsive">
                        <table class="table table-striped">
                            <thead>
                                <tr>
                                    <th>モデル名</th>
                                    <th>サイズ</th>
                                    <th>説明</th>
                                    <th>状態</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><code>cyberagent/calm3-22b-chat</code></td>
                                    <td>22B</td>
                                    <td>CyberAgent CALM3 日本語チャットモデル</td>
                                    <td><span class="model-status status-available">利用可能</span></td>
                                </tr>
                                <tr>
                                    <td><code>Qwen/Qwen2.5-14B-Instruct</code></td>
                                    <td>14B</td>
                                    <td>Qwen 2.5 14B 指示調整モデル</td>
                                    <td><span class="model-status status-available">利用可能</span></td>
                                </tr>
                                <tr>
                                    <td><code>Qwen/Qwen2.5-32B-Instruct</code></td>
                                    <td>32B</td>
                                    <td>Qwen 2.5 32B 指示調整モデル</td>
                                    <td><span class="model-status status-available">利用可能</span></td>
                                </tr>
                                <tr>
                                    <td><code>cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese</code></td>
                                    <td>32B</td>
                                    <td>DeepSeek R1 日本語特化モデル</td>
                                    <td><span class="model-status status-available">利用可能</span></td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <h3>未ダウンロードモデル（初回使用時にダウンロード）</h3>
                    <p>以下のモデルは選択可能ですが、初回使用時に自動的にダウンロードされます：</p>
                    <ul>
                        <li><strong>小規模モデル (82MB-3.6B)</strong>: distilgpt2、gpt2、Rinnaシリーズ、StableLM等</li>
                        <li><strong>中規模モデル (7B-13B)</strong>: ELYZA Llama等</li>
                        <li><strong>大規模モデル (17B-70B)</strong>: Meta Llama 3.1、Microsoft Phi-3.5等</li>
                    </ul>

                    <div class="info-box">
                        <i class="bi bi-info-circle"></i> <strong>ヒント</strong><br>
                        大規模モデル（17B以上）は数十GBのダウンロードが必要なため、事前にコマンドラインでダウンロードすることを推奨します。
                    </div>
                </section>

                <section id="model-download" class="manual-section">
                    <h2><i class="bi bi-download"></i> モデルのダウンロード方法</h2>
                    
                    <h3>自動ダウンロード</h3>
                    <p>Webインターフェースでモデルを選択すると、未ダウンロードのモデルは自動的にダウンロードされます。</p>
                    
                    <h3>手動ダウンロード（推奨）</h3>
                    <p>大規模モデルは以下のコマンドで事前にダウンロードできます：</p>
                    <pre class="bg-light p-3"><code># Dockerコンテナに入る
docker exec -it ai-ft-container bash

# Hugging Face CLIでダウンロード
huggingface-cli download meta-llama/Llama-3.1-17B-Instruct --local-dir ./models/Llama-3.1-17B</code></pre>

                    <h3>必要なディスク容量</h3>
                    <ul>
                        <li>17Bモデル: 約35GB</li>
                        <li>22Bモデル: 約44GB</li>
                        <li>32Bモデル: 約65GB</li>
                        <li>70Bモデル: 約140GB</li>
                    </ul>
                </section>

                <section id="finetuning" class="manual-section">
                    <h2><i class="bi bi-gear"></i> ファインチューニング手順</h2>
                    
                    <h3>1. データの準備</h3>
                    <p>JSONL形式でトレーニングデータを準備します：</p>
                    <pre class="bg-light p-3"><code>{"text": "質問: 日本の首都はどこですか？\n回答: 日本の首都は東京です。"}
{"text": "質問: 富士山の高さは何メートルですか？\n回答: 富士山の高さは3,776メートルです。"}</code></pre>

                    <h3>2. データのアップロード</h3>
                    <ol>
                        <li>「データ管理」タブを開く</li>
                        <li>「ファイルを選択」でJSONLファイルを選択</li>
                        <li>「アップロード」ボタンをクリック</li>
                    </ol>

                    <h3>3. ファインチューニングの実行</h3>
                    <ol>
                        <li>「ファインチューニング」タブを開く</li>
                        <li>ベースモデルを選択（ダウンロード済みモデルを推奨）</li>
                        <li>学習方法を選択（LoRA推奨）</li>
                        <li>パラメータを設定</li>
                        <li>「トレーニング開始」をクリック</li>
                    </ol>

                    <h3>推奨設定</h3>
                    <div class="table-responsive">
                        <table class="table">
                            <thead>
                                <tr>
                                    <th>モデルサイズ</th>
                                    <th>学習方法</th>
                                    <th>バッチサイズ</th>
                                    <th>学習率</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>3B-7B</td>
                                    <td>LoRA</td>
                                    <td>4-8</td>
                                    <td>3e-4</td>
                                </tr>
                                <tr>
                                    <td>14B-22B</td>
                                    <td>QLoRA (8bit)</td>
                                    <td>2-4</td>
                                    <td>2e-4</td>
                                </tr>
                                <tr>
                                    <td>32B以上</td>
                                    <td>QLoRA (4bit)</td>
                                    <td>1-2</td>
                                    <td>2e-4</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </section>

                <section id="text-generation" class="manual-section">
                    <h2><i class="bi bi-chat-dots"></i> テキスト生成</h2>
                    
                    <h3>手順</h3>
                    <ol>
                        <li>「テキスト生成」タブを開く</li>
                        <li>ファインチューニング済みモデルまたはベースモデルを選択</li>
                        <li>プロンプトを入力</li>
                        <li>生成パラメータを調整（任意）</li>
                        <li>「生成」ボタンをクリック</li>
                    </ol>

                    <h3>生成パラメータの説明</h3>
                    <ul>
                        <li><strong>最大長</strong>: 生成するテキストの最大トークン数</li>
                        <li><strong>温度</strong>: 出力の多様性（0.1-2.0、高いほど多様）</li>
                        <li><strong>Top-p</strong>: 確率の累積値によるフィルタリング（0.1-1.0）</li>
                    </ul>
                </section>

                <section id="troubleshooting" class="manual-section">
                    <h2><i class="bi bi-question-circle"></i> トラブルシューティング</h2>
                    
                    <h3>モデルのダウンロードが遅い</h3>
                    <ul>
                        <li>ネットワーク接続を確認してください</li>
                        <li>コマンドラインから手動ダウンロードを試してください</li>
                        <li>HuggingFaceのミラーサイトを使用することも可能です</li>
                    </ul>

                    <h3>メモリ不足エラー</h3>
                    <ul>
                        <li>より小さいモデルを使用してください</li>
                        <li>バッチサイズを小さくしてください</li>
                        <li>量子化（QLoRA）を使用してください</li>
                    </ul>

                    <h3>GPU関連のエラー</h3>
                    <ul>
                        <li>GPU使用状況を確認: <code>nvidia-smi</code></li>
                        <li>CUDAが正しくインストールされているか確認</li>
                        <li>必要に応じてGPUメモリをクリア</li>
                    </ul>

                    <div class="info-box">
                        <i class="bi bi-envelope"></i> <strong>サポート</strong><br>
                        問題が解決しない場合は、<a href="https://github.com/kji-furuta/AI_FT/issues">GitHubのIssues</a>でお問い合わせください。
                    </div>
                </section>
            </div>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>