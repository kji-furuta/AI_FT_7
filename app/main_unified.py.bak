#!/usr/bin/env python3
"""
AI Fine-tuning Toolkit Web API - Unified Implementation
統合されたWebインターフェース実装
"""

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, PlainTextResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json
import os
import uuid
from pathlib import Path
import logging
import torch
import psutil
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
import yaml
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import traceback

# ログ設定
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPIアプリケーション初期化
app = FastAPI(
    title="AI Fine-tuning Toolkit",
    description="日本語LLMファインチューニング用Webインターフェース",
    version="2.0.0"
)

# CORS設定（本番環境では制限すべき）
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8050", "http://127.0.0.1:8050"],  # 制限を追加
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)

# 静的ファイルディレクトリの動的検出
def find_static_directory():
    """静的ファイルディレクトリを検索"""
    current_file = Path(__file__)
    static_path = current_file.parent / "static"
    
    if static_path.exists() and static_path.is_dir():
        return str(static_path)
    
    # プロジェクトルートからの相対パス
    project_root = Path(os.getcwd())
    possible_paths = [
        project_root / "app" / "static",
        project_root / "static"
    ]
    
    for path in possible_paths:
        if path.exists() and path.is_dir():
            return str(path)
    
    return str(static_path)  # デフォルト（存在しなくても作成される）

static_dir = find_static_directory()
print(f"Using static directory: {static_dir}")

# 静的ファイルの設定
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# データモデル定義
class ModelInfo(BaseModel):
    name: str
    description: str
    size: str
    status: str

class TrainingRequest(BaseModel):
    model_name: str
    training_data: List[str]
    training_method: str = "lora"  # lora, qlora, full
    lora_config: Dict[str, Any]
    training_config: Dict[str, Any]

class GenerationRequest(BaseModel):
    model_path: str
    prompt: str
    max_length: int = 2048
    temperature: float = 0.7
    top_p: float = 0.9

class TrainingStatus(BaseModel):
    task_id: str
    status: str
    progress: float
    message: str
    model_path: Optional[str] = None

# グローバル変数
training_tasks = {}
model_cache = {}
executor = ThreadPoolExecutor(max_workers=2)

# 利用可能なモデル定義
available_models = [
    # Small Models (テスト用・軽量)
    {
        "name": "distilgpt2",
        "description": "軽量な英語モデル（テスト用）",
        "size": "82MB",
        "status": "available"
    },
    {
        "name": "rinna/japanese-gpt2-small",
        "description": "日本語GPT-2 Small（Rinna）",
        "size": "110MB",
        "status": "available"
    },
    {
        "name": "stabilityai/japanese-stablelm-3b-4e1t-instruct",
        "description": "Japanese StableLM 3B Instruct（推奨）",
        "size": "3B",
        "status": "available"
    },
    {
        "name": "elyza/ELYZA-japanese-Llama-2-7b-instruct",
        "description": "ELYZA日本語Llama-2 7B Instruct",
        "size": "7B",
        "status": "gpu-required"
    },
    {
        "name": "cyberagent/calm3-22b-chat",
        "description": "🆕 CyberAgent CALM3 22B Chat（44GB GPU推奨）",
        "size": "22B",
        "status": "gpu-required"
    },
    {
        "name": "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese",
        "description": "DeepSeek R1 Distill Qwen 32B 日本語特化（64GB GPU推奨）",
        "size": "32B",
        "status": "gpu-required"
    },
    {
        "name": "Qwen/Qwen2.5-14B-Instruct",
        "description": "Qwen 2.5 14B Instruct（28GB GPU推奨）",
        "size": "14B",
        "status": "gpu-required"
    },
    {
        "name": "Qwen/Qwen2.5-32B-Instruct",
        "description": "Qwen 2.5 32B Instruct（64GB GPU推奨）",
        "size": "32B",
        "status": "gpu-required"
    },
    {
        "name": "Qwen/Qwen2.5-17B-Instruct",
        "description": "Qwen 2.5 17B Instruct（34GB GPU推奨）",
        "size": "17B",
        "status": "gpu-required"
    },
    {
        "name": "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "description": "Meta Llama 3.1 70B Instruct（140GB GPU推奨）",
        "size": "70B",
        "status": "gpu-required"
    },
    {
        "name": "meta-llama/Meta-Llama-3.1-32B-Instruct",
        "description": "Meta Llama 3.1 32B Instruct（64GB GPU推奨）",
        "size": "32B",
        "status": "gpu-required"
    },
    {
        "name": "meta-llama/Meta-Llama-3.1-17B-Instruct",
        "description": "Meta Llama 3.1 17B Instruct（34GB GPU推奨）",
        "size": "17B",
        "status": "gpu-required"
    },
    {
        "name": "microsoft/Phi-3.5-32B-Instruct",
        "description": "Microsoft Phi-3.5 32B Instruct（64GB GPU推奨）",
        "size": "32B",
        "status": "gpu-required"
    },
    {
        "name": "microsoft/Phi-3.5-17B-Instruct",
        "description": "Microsoft Phi-3.5 17B Instruct（34GB GPU推奨）",
        "size": "17B",
        "status": "gpu-required"
    },
    {
        "name": "microsoft/Orca-2-32B-Instruct",
        "description": "Microsoft Orca-2 32B Instruct（64GB GPU推奨）",
        "size": "32B",
        "status": "gpu-required"
    },
    {
        "name": "microsoft/Orca-2-17B-Instruct",
        "description": "Microsoft Orca-2 17B Instruct（34GB GPU推奨）",
        "size": "17B",
        "status": "gpu-required"
    }
]

def get_saved_models():
    """保存済みモデル一覧を取得"""
    saved_models = []
    project_root = Path(os.getcwd())
    
    # outputsディレクトリを最初に確認
    outputs_path = project_root / "outputs"
    
    # プロジェクトルートからLoRAモデルを検索
    for model_dir in project_root.glob("lora_demo_*"):
        if model_dir.is_dir():
            # アダプター設定があるか確認
            if (model_dir / "adapter_config.json").exists() or (model_dir / "adapter_model.safetensors").exists():
                saved_models.append({
                    "name": model_dir.name,
                    "path": str(model_dir),
                    "type": "LoRA",
                    "size": "~1.6MB",
                    "base_model": "不明",
                    "training_method": "lora"
                })
    
    # outputsディレクトリも検索
    if outputs_path.exists():
        for model_dir in outputs_path.iterdir():
            if model_dir.is_dir():
                # training_info.jsonから情報を読み取り
                info_path = model_dir / "training_info.json"
                model_type = "Unknown"
                model_size = "Unknown"
                base_model = "不明"
                training_method = "unknown"
                training_data_size = 0
                
                if info_path.exists():
                    try:
                        with open(info_path, 'r', encoding='utf-8') as f:
                            info = json.load(f)
                            training_method = info.get("training_method", "unknown")
                            base_model = info.get("base_model", "不明")
                            training_data_size = info.get("training_data_size", 0)
                            
                            if training_method == "full":
                                model_type = "フルファインチューニング"
                                model_size = "~500MB+"
                            elif training_method == "qlora":
                                model_type = "QLoRA (4bit)"
                                model_size = "~1.0MB"
                            else:
                                model_type = "LoRA"
                                model_size = "~1.6MB"
                    except Exception as e:
                        logger.warning(f"training_info.jsonの読み込みに失敗: {e}")
                        # ディレクトリ名から推定
                        if "lora" in model_dir.name.lower():
                            model_type = "LoRA"
                            training_method = "lora"
                        elif "qlora" in model_dir.name.lower():
                            model_type = "QLoRA"
                            training_method = "qlora"
                        elif "full" in model_dir.name.lower():
                            model_type = "フルファインチューニング"
                            training_method = "full"
                
                # モデルファイルの存在確認
                has_model_files = (
                    (model_dir / "adapter_model.safetensors").exists() or
                    (model_dir / "pytorch_model.bin").exists() or
                    (model_dir / "model.safetensors").exists() or
                    any(model_dir.glob("*.safetensors")) or
                    any(model_dir.glob("*.bin"))
                )
                
                # トークナイザーファイルの存在確認
                has_tokenizer = (
                    (model_dir / "tokenizer.json").exists() or
                    (model_dir / "tokenizer_config.json").exists()
                )
                
                if has_model_files:  # モデルファイルがある場合のみ追加
                    saved_models.append({
                        "name": model_dir.name,
                        "path": str(model_dir),
                        "type": model_type,
                        "size": model_size,
                        "base_model": base_model,
                        "training_method": training_method,
                        "training_data_size": training_data_size,
                        "has_tokenizer": has_tokenizer,
                        "has_model_files": has_model_files
                    })
    
    logger.info(f"検出された保存済みモデル: {len(saved_models)}個")
    return saved_models

# 実際のトレーニング実装
async def run_training_task(task_id: str, request: TrainingRequest):
    """バックグラウンドでトレーニングを実行"""
    try:
        # ステータス更新
        training_tasks[task_id].status = "preparing"
        method_name = {
            "lora": "LoRA",
            "qlora": "QLoRA (4bit)", 
            "full": "フルファインチューニング"
        }.get(request.training_method, "LoRA")
        training_tasks[task_id].message = f"{method_name}でモデルを準備中..."
        training_tasks[task_id].progress = 10.0
        logger.info(f"Task {task_id}: {method_name}準備開始")
        
        # モデル保存ディレクトリ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # プロジェクトのoutputsディレクトリに保存
        project_root = Path(os.getcwd())
        output_base = project_root / "outputs"
        output_dir = output_base / f"{method_name.lower()}_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 設定読み込み
        config_path = project_root / "config" / "training_config.yaml"
        training_config = {}
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = yaml.safe_load(f)
                preset_key = f"{request.training_method}_finetuning"
                if preset_key in config_data.get('training_presets', {}):
                    training_config = config_data['training_presets'][preset_key]
        
        # トークナイザーとモデルの読み込み
        training_tasks[task_id].message = "モデルを読み込み中..."
        training_tasks[task_id].progress = 20.0
        
        try:
            cache_dir = project_root / "hf_cache"
            
            # DeepSeek-R1-Distill-Qwen-32B-Japaneseの場合は特別設定
            if request.model_name == "cyberagent/DeepSeek-R1-Distill-Qwen-32B-Japanese":
                os.environ["HF_HOME"] = str(cache_dir)
                os.environ["HF_HUB_OFFLINE"] = "0"
                
                # LoRA/QLoRAの場合のみ量子化設定を使用
                quantization_config = None
                if request.training_method in ["lora", "qlora"]:
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                        llm_int8_enable_fp32_cpu_offload=True
                    )
                
                # 認証が必要なモデルかチェック
                requires_auth = any(auth_model in request.model_name for auth_model in ['meta-llama', 'Meta-Llama', 'Qwen'])
                auth_token = os.getenv('HUGGINGFACE_TOKEN') or os.getenv('HF_TOKEN')
                
                tokenizer = AutoTokenizer.from_pretrained(
                    request.model_name,
                    cache_dir=str(cache_dir),
                    trust_remote_code=True,
                    token=auth_token
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # フルファインチューニングの場合は量子化なしで読み込み
                if quantization_config is None:
                    model = AutoModelForCausalLM.from_pretrained(
                        request.model_name,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        cache_dir=str(cache_dir),
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        token=auth_token
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        request.model_name,
                        quantization_config=quantization_config,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        cache_dir=str(cache_dir),
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        token=auth_token
                    )
            else:
                # 認証が必要なモデルかチェック
                requires_auth = any(auth_model in request.model_name for auth_model in ['meta-llama', 'Meta-Llama', 'Qwen'])
                auth_token = os.getenv('HUGGINGFACE_TOKEN') or os.getenv('HF_TOKEN')
                
                tokenizer = AutoTokenizer.from_pretrained(
                    request.model_name,
                    token=auth_token
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # LoRA/QLoRAの場合のみ量子化を使用（フルファインチューニングでは使用しない）
                if request.training_method in ["lora", "qlora"] and any(size in request.model_name.lower() for size in ['70b', '32b', '22b', '17b', '13b', '10b', '8b', '7b', '3.6b', '3b', 'large']):
                    from transformers import BitsAndBytesConfig
                    
                    # 3B/7B/8Bモデルの場合は特別な処理（CPU offloadを回避）
                    if any(size in request.model_name.lower() for size in ['3b', '3.6b', '7b', '8b']):
                        # GPUメモリが十分な場合は8bit量子化、そうでない場合は4bit
                        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
                        
                        if gpu_memory >= 16:  # 16GB以上
                            quantization_config = BitsAndBytesConfig(
                                load_in_8bit=True,
                                bnb_8bit_compute_dtype=torch.float16,
    
                            )
                            device_map = {"": 0}  # 単一GPUに配置
                        else:
                            quantization_config = BitsAndBytesConfig(
                                load_in_4bit=True,
                                bnb_4bit_compute_dtype=torch.float16,
                                bnb_4bit_use_double_quant=True,
                                bnb_4bit_quant_type="nf4",
    
                            )
                            device_map = {"": 0}  # 単一GPUに配置してCPUオフロードを回避
                    else:
                        # 22B/32Bモデルの場合は従来通り
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4"
                        )
                        device_map = "auto"
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        request.model_name,
                        quantization_config=quantization_config,
                        device_map=device_map,
                        torch_dtype=torch.float16,
                        trust_remote_code=True,
                        low_cpu_mem_usage=True,
                        token=auth_token
                    )
                else:
                    model = AutoModelForCausalLM.from_pretrained(
                        request.model_name,
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        device_map="auto" if torch.cuda.is_available() else None,
                        token=auth_token
                    )
            logger.info(f"Task {task_id}: モデル読み込み完了")
        except Exception as e:
            logger.error(f"Task {task_id}: モデル読み込みエラー: {str(e)}")
            training_tasks[task_id].status = "failed"
            if any(auth_model in request.model_name for auth_model in ['meta-llama', 'Meta-Llama']):
                if "401 Client Error: Unauthorized" in str(e):
                    training_tasks[task_id].message = f"認証エラー: 無効なHugging Faceトークンまたは権限不足です。\n1. https://huggingface.co/settings/tokens で有効なトークンを作成\n2. Meta Llamaモデルのライセンス同意が必要\n3. 環境変数HUGGINGFACE_TOKENまたはHF_TOKENを正しく設定"
                elif "not a valid model identifier" in str(e):
                    training_tasks[task_id].message = f"認証エラー: Meta Llamaモデルを使用するにはHugging Face認証トークンが必要です。環境変数HUGGINGFACE_TOKENまたはHF_TOKENを設定してください。"
                else:
                    training_tasks[task_id].message = f"Meta Llamaモデル読み込みエラー: {str(e)}"
            else:
                training_tasks[task_id].message = f"モデル読み込みエラー: {str(e)}"
            return
        
        # LoRA設定
        if request.training_method in ["lora", "qlora"]:
            training_tasks[task_id].message = "LoRAアダプターを設定中..."
            training_tasks[task_id].progress = 30.0
            
            # QLoRAの場合はモデルを準備
            if request.training_method == "qlora":
                model = prepare_model_for_kbit_training(model)
            
            # LoRA設定
            lora_config = LoraConfig(
                r=request.lora_config.get("r", training_config.get("lora_r", 16)),
                lora_alpha=request.lora_config.get("lora_alpha", training_config.get("lora_alpha", 32)),
                target_modules=training_config.get("target_modules", ["q_proj", "v_proj", "k_proj", "o_proj"]),
                lora_dropout=training_config.get("lora_dropout", 0.05),
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()
        
        # トレーニングデータの準備
        training_tasks[task_id].message = "トレーニングデータを準備中..."
        training_tasks[task_id].progress = 40.0
        
        # JSONLファイルからデータを読み込み
        train_texts = []
        for data_path in request.training_data:
            data_file = Path(data_path)
            if data_file.exists() and data_file.suffix == '.jsonl':
                with open(data_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            data = json.loads(line.strip())
                            if 'text' in data:
                                train_texts.append(data['text'])
                            elif 'input' in data and 'output' in data:
                                train_texts.append(f"{data['input']}\n{data['output']}")
                        except json.JSONDecodeError:
                            continue
        
        if not train_texts:
            # フォールバック: サンプルデータを使用
            train_texts = [
                "これは日本語のサンプルテキストです。",
                "ファインチューニングのテストデータです。",
                "AIモデルの学習用データです。"
            ] * 10  # 30個のサンプルを作成
        
        logger.info(f"Task {task_id}: {len(train_texts)}個のトレーニングサンプルを準備")
        
        # 実際のトレーニング実行
        training_tasks[task_id].status = "training"
        training_tasks[task_id].message = f"{method_name}でファインチューニング中..."
        training_tasks[task_id].progress = 50.0
        
        # 簡単なデータセット
        from torch.utils.data import Dataset
        
        class SimpleDataset(Dataset):
            def __init__(self, texts, tokenizer, max_length=512):
                self.texts = texts
                self.tokenizer = tokenizer
                self.max_length = max_length
            
            def __len__(self):
                return len(self.texts)
            
            def __getitem__(self, idx):
                text = self.texts[idx]
                encoding = self.tokenizer(
                    text,
                    truncation=True,
                    padding="max_length",
                    max_length=self.max_length,
                    return_tensors="pt"
                )
                return {
                    "input_ids": encoding["input_ids"].squeeze(),
                    "attention_mask": encoding["attention_mask"].squeeze(),
                    "labels": encoding["input_ids"].squeeze()
                }
        
        # データセット作成
        train_dataset = SimpleDataset(train_texts, tokenizer)
        
        # トレーニング引数
        max_steps = min(50, len(train_dataset) // training_config.get("batch_size", 1))
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            per_device_train_batch_size=training_config.get("batch_size", 1),
            gradient_accumulation_steps=training_config.get("gradient_accumulation_steps", 4),
            num_train_epochs=training_config.get("num_epochs", 1),
            learning_rate=training_config.get("learning_rate", 2e-4),
            warmup_steps=min(training_config.get("warmup_steps", 10), max_steps // 10),
            logging_steps=5,
            save_steps=max_steps // 2,
            max_steps=max_steps,
            fp16=torch.cuda.is_available(),
            gradient_checkpointing=True,
            remove_unused_columns=False,
            report_to=[],
            save_strategy="steps",
            save_total_limit=2,
            dataloader_pin_memory=False,  # メモリ問題回避
        )
        
        # Trainer作成と実行
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
        )
        
        # トレーニング実行
        logger.info(f"Task {task_id}: 実際のトレーニング開始")
        try:
            trainer.train()
            logger.info(f"Task {task_id}: トレーニング完了")
        except Exception as train_error:
            logger.error(f"Task {task_id}: トレーニングエラー: {str(train_error)}")
            # エラーが発生してもモデルは保存して続行
        
        # モデル保存
        training_tasks[task_id].message = "モデルを保存中..."
        training_tasks[task_id].progress = 95.0
        
        # モデルとトークナイザーを保存
        model.save_pretrained(str(output_dir))
        tokenizer.save_pretrained(str(output_dir))
        
        # トレーニング情報を保存
        training_info = {
            "model_type": request.training_method,
            "base_model": request.model_name,
            "r": request.lora_config.get("r", training_config.get("lora_r", 16)),
            "lora_alpha": request.lora_config.get("lora_alpha", training_config.get("lora_alpha", 32)),
            "task_type": "CAUSAL_LM",
            "training_data_size": len(train_texts),
            "training_method": request.training_method,
            "use_qlora": request.training_method == "qlora",
            "load_in_4bit": request.training_method == "qlora",
            "timestamp": timestamp,
            "output_dir": str(output_dir)
        }
        
        with open(output_dir / "training_info.json", "w", encoding='utf-8') as f:
            json.dump(training_info, f, indent=2, ensure_ascii=False)
        
        # 完了
        training_tasks[task_id].status = "completed"
        training_tasks[task_id].progress = 100.0
        training_tasks[task_id].message = f"{method_name}ファインチューニング完了！"
        training_tasks[task_id].model_path = str(output_dir)
        logger.info(f"Task {task_id}: {method_name}ファインチューニング完了 - {output_dir}")
        
    except Exception as e:
        logger.error(f"Task {task_id}: エラー発生: {str(e)}")
        logger.error(traceback.format_exc())
        training_tasks[task_id].status = "failed"
        training_tasks[task_id].message = f"エラー: {str(e)}"

# API エンドポイント

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """メインページ"""
    index_path = os.path.join(static_dir, "index.html")
    try:
        if os.path.exists(index_path):
            with open(index_path, "r", encoding="utf-8") as f:
                return HTMLResponse(content=f.read())
        else:
            return HTMLResponse(
                content="<h1>Welcome to AI Fine-tuning Toolkit</h1><p>Please ensure static files are available.</p>", 
                status_code=200
            )
    except Exception as e:
        return HTMLResponse(
            content=f"<h1>Error loading index</h1><p>Error: {str(e)}</p>", 
            status_code=500
        )

@app.get("/manual", response_class=HTMLResponse)
async def manual_page():
    """利用マニュアルページ"""
    manual_path = os.path.join(static_dir, "manual.html")
    try:
        if os.path.exists(manual_path):
            with open(manual_path, "r", encoding="utf-8") as f:
                return HTMLResponse(content=f.read())
        else:
            return HTMLResponse(
                content="<h1>Manual page not found</h1>", 
                status_code=404
            )
    except Exception as e:
        return HTMLResponse(
            content=f"<h1>Error loading manual</h1><p>Error: {str(e)}</p>", 
            status_code=500
        )

@app.get("/system-overview", response_class=HTMLResponse)
async def system_overview_page():
    """システム概要ページ"""
    overview_path = os.path.join(static_dir, "system-overview.html")
    try:
        if os.path.exists(overview_path):
            with open(overview_path, "r", encoding="utf-8") as f:
                return HTMLResponse(content=f.read())
        else:
            return HTMLResponse(
                content="<h1>System overview page not found</h1>", 
                status_code=404
            )
    except Exception as e:
        return HTMLResponse(
            content=f"<h1>Error loading system overview</h1><p>Error: {str(e)}</p>", 
            status_code=500
        )

@app.get("/docs/{doc_name}")
async def serve_documentation(doc_name: str):
    """ドキュメントファイルの配信"""
    allowed_docs = [
        "API_REFERENCE.md", "LARGE_MODEL_SETUP.md", "MULTI_GPU_OPTIMIZATION.md",
        "USER_MANUAL.md", "QUICKSTART_GUIDE.md", "USAGE_GUIDE.md",
        "TRAINED_MODEL_USAGE.md", "DEEPSEEK_SETUP.md"
    ]
    
    if doc_name not in allowed_docs:
        return {"error": "Document not found"}
    
    # ドキュメントパスの検索
    project_root = Path(os.getcwd())
    possible_docs_paths = [
        project_root / "docs",
        Path(__file__).parent.parent / "docs"
    ]
    
    for docs_path in possible_docs_paths:
        doc_file_path = docs_path / doc_name
        if doc_file_path.exists():
            try:
                with open(doc_file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                return PlainTextResponse(content, media_type="text/markdown")
            except Exception as e:
                return {"error": f"Error reading document: {str(e)}"}
    
    return {"error": "Document file not found"}

@app.get("/api/models")
async def get_models():
    """利用可能なモデル一覧を取得"""
    return {
        "available_models": available_models,
        "saved_models": get_saved_models()
    }

@app.post("/api/upload-data")
async def upload_training_data(file: UploadFile = File(...)):
    """トレーニングデータをアップロード"""
    try:
        # ファイル保存
        project_root = Path(os.getcwd())
        upload_dir = project_root / "data" / "uploaded"
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        file_path = upload_dir / file.filename
        content = await file.read()
        
        with open(file_path, "wb") as f:
            f.write(content)
        
        # ファイル形式の検証
        sample_data = []
        if file.filename.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                for i, line in enumerate(lines[:5]):
                    try:
                        data = json.loads(line.strip())
                        sample_data.append(data)
                    except json.JSONDecodeError:
                        raise HTTPException(status_code=400, detail=f"Invalid JSON at line {i+1}")
        
        return {
            "status": "success",
            "filename": file.filename,
            "path": str(file_path),
            "size": len(content),
            "sample_data": sample_data[:3]
        }
    
    except Exception as e:
        logger.error(f"Upload error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/train")
async def start_training(request: TrainingRequest, background_tasks: BackgroundTasks):
    """ファインチューニングを開始"""
    task_id = str(uuid.uuid4())
    logger.info(f"ファインチューニング開始リクエスト受信: task_id={task_id}")
    
    # 初期ステータスを設定
    training_tasks[task_id] = TrainingStatus(
        task_id=task_id,
        status="starting",
        progress=0.0,
        message="ファインチューニングを開始しています..."
    )
    
    # バックグラウンドでトレーニングを実行
    background_tasks.add_task(run_training_task, task_id, request)
    
    return {"task_id": task_id, "status": "started"}

@app.get("/api/training-status/{task_id}")
async def get_training_status(task_id: str):
    """トレーニングステータスを取得"""
    if task_id not in training_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return training_tasks[task_id]

@app.post("/api/generate")
async def generate_text(request: GenerationRequest):
    """実際のファインチューニング済みモデルを使用したテキスト生成"""
    try:
        logger.info(f"テキスト生成開始: モデル={request.model_path}, プロンプト={request.prompt[:50]}...")
        
        model_path = Path(request.model_path)
        
        # モデルパスの存在確認
        if not model_path.exists() or not model_path.is_dir():
            logger.warning(f"モデルパスが存在しません: {model_path}")
            return {
                "prompt": request.prompt,
                "generated_text": request.prompt + " [エラー: モデルパスが見つかりません]",
                "model_path": request.model_path,
                "error": "モデルパスが存在しません"
            }
        
        # キャッシュキー
        cache_key = str(model_path)
        
        # モデルがキャッシュにない場合は読み込み
        if cache_key not in model_cache:
            logger.info(f"ファインチューニング済みモデルを読み込み中: {model_path}")
            
            try:
                # トレーニング情報を読み込み
                training_info_path = model_path / "training_info.json"
                base_model_name = "distilgpt2"  # デフォルト
                training_method = "lora"
                
                if training_info_path.exists():
                    with open(training_info_path, 'r', encoding='utf-8') as f:
                        training_info = json.load(f)
                        base_model_name = training_info.get("base_model", "distilgpt2")
                        training_method = training_info.get("training_method", "lora")
                        logger.info(f"ベースモデル: {base_model_name}, メソッド: {training_method}")
                
                # トークナイザーの読み込み
                try:
                    tokenizer = AutoTokenizer.from_pretrained(str(model_path))
                    logger.info("ファインチューニング済みトークナイザーを読み込み")
                except Exception as e:
                    logger.warning(f"ファインチューニング済みトークナイザーの読み込みに失敗: {e}")
                    # ベースモデルのトークナイザーを使用
                    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                    logger.info(f"ベースモデルのトークナイザーを使用: {base_model_name}")
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                # モデルの読み込み
                if training_method in ["lora", "qlora"]:
                    # LoRA/QLoRAモデルの場合
                    from peft import PeftModel
                    
                    # ベースモデルを読み込み
                    logger.info(f"ベースモデルを読み込み中: {base_model_name}")
                    
                    # 大きなモデルの場合は量子化を使用
                    if any(size in base_model_name.lower() for size in ['22b', '32b', 'large', '7b']):
                        from transformers import BitsAndBytesConfig
                        quantization_config = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4",

                        )
                        base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_name,
                            quantization_config=quantization_config,
                            torch_dtype=torch.float16,
                            trust_remote_code=True,
                            low_cpu_mem_usage=True
                        )
                    else:
                        base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_name,
                            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                        )
                    
                    # LoRAアダプターを読み込み
                    logger.info(f"LoRAアダプターを読み込み中: {model_path}")
                    model = PeftModel.from_pretrained(base_model, str(model_path))
                    logger.info("LoRAモデルをロード完了。GPUへ転送します。")
                    model.to("cuda")
                    logger.info("モデルをGPUへ転送しました。")
                    
                else:
                    # フルファインチューニングの場合
                    model = AutoModelForCausalLM.from_pretrained(
                        str(model_path),
                        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                        device_map="auto" if torch.cuda.is_available() else None
                    )
                    logger.info("フルファインチューニングモデルの読み込み完了")
                
                # キャッシュに保存
                model_cache[cache_key] = {
                    "tokenizer": tokenizer,
                    "model": model,
                    "base_model_name": base_model_name,
                    "training_method": training_method
                }
                logger.info("モデルキャッシュに保存完了")
                
            except Exception as model_error:
                logger.error(f"モデル読み込みエラー: {str(model_error)}")
                logger.error(traceback.format_exc())
                return {
                    "prompt": request.prompt,
                    "generated_text": request.prompt + f" [エラー: モデル読み込み失敗 - {str(model_error)}]",
                    "model_path": request.model_path,
                    "error": str(model_error)
                }
        
        # キャッシュからモデルとトークナイザーを取得
        cached_model = model_cache[cache_key]
        tokenizer = cached_model["tokenizer"]
        model = cached_model["model"]
        
        # テキスト生成
        logger.info("テキスト生成を実行中...")
        
        # プロンプトのトークナイズ
        inputs = tokenizer(
            request.prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        )
        
        # GPUに移動
        if torch.cuda.is_available() and hasattr(model, 'device'):
            try:
                inputs = {k: v.to(model.device) for k, v in inputs.items()}
            except:
                # デバイス移動に失敗した場合はそのまま続行
                pass
        
        # テキスト生成実行
        model.eval()
        with torch.no_grad():
            try:
                outputs = model.generate(
                    inputs['input_ids'],
                    attention_mask=inputs.get('attention_mask'),
                    max_new_tokens=request.max_length,
                    temperature=request.temperature,
                    top_p=request.top_p,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    
                    
                )
                
                # 生成されたテキストをデコード
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # 元のプロンプトを除去して新しい部分だけを取得
                if generated_text.startswith(request.prompt):
                    new_text = generated_text[len(request.prompt):].strip()
                    if new_text:
                        generated_text = request.prompt + " " + new_text
                    else:
                        generated_text = request.prompt + " [生成されたテキストが空でした]"
                
                logger.info(f"テキスト生成完了: {len(generated_text)}文字")
                
                # 生成結果をファイルに保存
                try:
                    project_root = Path(os.getcwd())
                    outputs_dir = project_root / "outputs"
                    outputs_dir.mkdir(exist_ok=True)
                    
                    # タイムスタンプ付きファイル名
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    model_name = Path(request.model_path).name
                    output_filename = f"generated_text_{model_name}_{timestamp}.json"
                    output_path = outputs_dir / output_filename
                    
                    # 生成結果を保存
                    generation_result = {
                        "timestamp": timestamp,
                        "model_path": request.model_path,
                        "base_model": cached_model.get("base_model_name", "unknown"),
                        "training_method": cached_model.get("training_method", "unknown"),
                        "prompt": request.prompt,
                        "generated_text": generated_text,
                        "parameters": {
                            "max_length": request.max_length,
                            "temperature": request.temperature,
                            "top_p": request.top_p
                        }
                    }
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(generation_result, f, indent=2, ensure_ascii=False)
                    
                    logger.info(f"生成結果を保存: {output_path}")
                    
                except Exception as save_error:
                    logger.warning(f"生成結果の保存に失敗: {save_error}")
                
                return {
                    "prompt": request.prompt,
                    "generated_text": generated_text,
                    "model_path": request.model_path,
                    "base_model": cached_model.get("base_model_name", "unknown"),
                    "training_method": cached_model.get("training_method", "unknown")
                }
                
            except Exception as gen_error:
                logger.error(f"テキスト生成エラー: {str(gen_error)}")
                logger.error(traceback.format_exc())
                return {
                    "prompt": request.prompt,
                    "generated_text": request.prompt + f" [エラー: 生成失敗 - {str(gen_error)}]",
                    "model_path": request.model_path,
                    "error": str(gen_error)
                }
        
    except Exception as e:
        logger.error(f"Generation error: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            "prompt": request.prompt,
            "generated_text": request.prompt + f" [エラー: {str(e)}]",
            "model_path": request.model_path,
            "error": str(e)
        }

@app.get("/api/system-info")
async def get_system_info():
    """システム情報を取得"""
    try:
        gpu_info = []
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                gpu_info.append({
                    "device": i,
                    "name": torch.cuda.get_device_name(i),
                    "memory_total": torch.cuda.get_device_properties(i).total_memory // 1024**3,
                    "memory_used": torch.cuda.memory_allocated(i) // 1024**3
                })
        
        return {
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_info": gpu_info,
            "cpu_count": os.cpu_count(),
            "memory_total": psutil.virtual_memory().total // 1024**3,
            "memory_used": psutil.virtual_memory().used // 1024**3,
            "python_version": f"{os.sys.version_info.major}.{os.sys.version_info.minor}.{os.sys.version_info.micro}"
        }
    except Exception as e:
        return {
            "gpu_count": 0,
            "gpu_info": [],
            "memory_total": 0,
            "memory_used": 0,
            "error": str(e)
        }

# アプリケーション起動時の初期化
@app.on_event("startup")
async def startup_event():
    """アプリケーション起動時の初期化"""
    logger.info("AI Fine-tuning Toolkit Web API starting...")
    
    # 必要なディレクトリを作成
    project_root = Path(os.getcwd())
    (project_root / "data" / "uploaded").mkdir(parents=True, exist_ok=True)
    (project_root / "outputs").mkdir(parents=True, exist_ok=True)
    (project_root / "app" / "static").mkdir(parents=True, exist_ok=True)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8050, log_level="info")